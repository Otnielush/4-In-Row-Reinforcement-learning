{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/programming-datascience/d8b96346e347b0b6942e16a33e64039c#file-actor-critic-cartpole-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "# import gym\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple, deque\n",
    "from time import sleep\n",
    "# import supersuit as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# Importing PyTorch here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_0': Dict(action_mask:Box(0, 1, (7,), int8), observation:Box(0, 1, (6, 7, 2), int8)), 'player_1': Dict(action_mask:Box(0, 1, (7,), int8), observation:Box(0, 1, (6, 7, 2), int8))}\n",
      "{'player_0': Discrete(7), 'player_1': Discrete(7)}\n"
     ]
    }
   ],
   "source": [
    "env = connect_four_v3.env()\n",
    "# env = ss.resize_v0(env, x_size=84, y_size=84)\n",
    "print(env.observation_spaces)\n",
    "print(env.action_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can move either left or right to balance the pole\n",
    "# Lets implement the Actor critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, name, obs_shape, act_shape, buffer_size, lr=1e-2):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.name = name\n",
    "        self.games_played = 0\n",
    "        self.wins = 0\n",
    "        self.history = deque(maxlen=1000000)\n",
    "        self.conv1 = nn.Conv2d(2, 8, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 12, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(12)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.calc_input_size(), 64)\n",
    "        self.actor = nn.Linear(64, act_shape) \n",
    "        self.critic = nn.Linear(64, 1) # Critic is always 1\n",
    "        self.saved_actions = deque(maxlen=buffer_size)\n",
    "        self.rewards = deque(maxlen=buffer_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def calc_input_size(self):\n",
    "        m = self.conv1(torch.zeros((1,)+self.obs_shape))\n",
    "#         print(m.shape)\n",
    "        m = self.bn1(m)\n",
    "#         print(m.shape)\n",
    "        m = self.conv2(m)\n",
    "#         print(m.shape)\n",
    "        m = self.maxpool1(m)\n",
    "#         print(m.shape)\n",
    "        return int(np.prod(m.size()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x.view((1,)+self.obs_shape)))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = F.relu(self.fc1(x.reshape(-1, 12)))\n",
    "        action_prob = F.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_prob, state_values\n",
    "    \n",
    "    def select_action(self, state, mask):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        probs, state_value = self.forward(state)\n",
    "        mask = torch.from_numpy(mask)\n",
    "#         print(probs)\n",
    "        m = Categorical(probs * mask)\n",
    "        action = m.sample()\n",
    "#         action = torch.argmax(probs * mask)\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "        return action.item()\n",
    "        # In this function, we decide whehter we want the block to move left or right,based on what the model decided\n",
    "        \n",
    "    def finish_episode(self):\n",
    "        # We calculate the losses and perform backprop in this function\n",
    "        R = 0\n",
    "        saved_actions = [x for x in self.saved_actions]\n",
    "    #     log_prob = torch.tensor([x.log_prob for x in model.saved_actions])\n",
    "    #     value = \n",
    "        policy_losses = []\n",
    "        value_losses =[]\n",
    "        returns = []\n",
    "        rewards = [x for x in self.rewards]\n",
    "\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + 0.99 * R # 0.99 is our gamma number\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        for (log_prob, value), R in zip(saved_actions, returns):\n",
    "            advantage = R - value.item()\n",
    "\n",
    "            policy_losses.append(-log_prob * advantage)\n",
    "            value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "        loss.float().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    #     del model.rewards[:]\n",
    "    #     del model.saved_actions[:]\n",
    "        self.rewards.clear()\n",
    "        self.saved_actions.clear()\n",
    "    \n",
    "    def save(self, suff=''):\n",
    "        if len(suff) > 0:\n",
    "            suff = '_'+suff\n",
    "        torch.save(self.state_dict(), f\"Connect4_models/Connect4_{self.name}{suff}.pt\")\n",
    "        \n",
    "    def load(self, suff=''):\n",
    "        if len(suff) > 0:\n",
    "            suff = '_'+suff\n",
    "        self.load_state_dict(torch.load(f\"Connect4_models/Connect4_{self.name}{suff}.pt\"))\n",
    "        \n",
    "    def game_done(self, reward):\n",
    "        self.games_played += 1\n",
    "        self.wins += 1 if reward > 0 else 0\n",
    "        self.history.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes_max, t_max=1000):\n",
    "#     print('target reward:', env.spec.reward_threshold)\n",
    "    running_reward = 0\n",
    "    for i_episode in range(episodes_max): # We need around this much episodes\n",
    "        env.reset()\n",
    "        ep_reward = 0\n",
    "        reward = [0,0]\n",
    "        done = [0,0]\n",
    "        for t in range(t_max):\n",
    "            state, reward[t%2], done[t%2], _ = env.last()\n",
    "            reward[t%2] = float(reward[t%2])\n",
    "            players[t%2].rewards.append(reward[t%2])\n",
    "            if done[t%2]:\n",
    "                if all(done):\n",
    "                    players[t%2].game_done(reward[t%2])\n",
    "                    players[(t+1)%2].game_done(reward[(t+1)%2])\n",
    "                    break\n",
    "                env.step(None)\n",
    "                continue\n",
    "            action = players[t%2].select_action(state['observation'], state['action_mask'])\n",
    "#             ep_reward += reward\n",
    "            env.step(action)\n",
    "            \n",
    "            \n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "        model_1.finish_episode()\n",
    "        model_2.finish_episode()\n",
    "\n",
    "        print(\"\\rEpisode {}\\tmodel_1 wins: {:.2f}\\tmodel_2 wins: {:.2f}\".format(\n",
    "                i_episode, model_1.wins, model_2.wins\n",
    "            ), end=' '*10)\n",
    "        if i_episode % 100 == 0: # We will print some things out\n",
    "            print(\"\\rEpisode {}\\tmodel_1 wins: {:.2f}\\tmodel_2 wins: {:.2f}\".format(\n",
    "                i_episode, model_1.wins, model_2.wins\n",
    "            ), end=' '*10)\n",
    "            print()\n",
    "            model_1.save('last')\n",
    "            model_2.save('last')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = False\n",
    "buffer_size = 50000\n",
    "\n",
    "obs = env.observation_spaces['player_0'].spaces['observation'].shape\n",
    "mask = env.observation_spaces['player_0'].spaces['action_mask'].shape[0]\n",
    "# obs = obs[0] * obs[1] * obs[2] + mask\n",
    "\n",
    "model_1 = ActorCritic('model1', obs[::-1], env.action_spaces['player_0'].n, buffer_size, lr=1e-3)\n",
    "model_2 = ActorCritic('model2', obs[::-1], env.action_spaces['player_0'].n, buffer_size, lr=5e-3)\n",
    "players = [model_1, model_2]\n",
    "\n",
    "if saved_model:\n",
    "    model_1.load('last')\n",
    "    model_2.load('last')\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1920928955078125e-07"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-20b610ae000b>:79: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tmodel_1 wins: 0.00\tmodel_2 wins: 1.00          \n",
      "Episode 100\tmodel_1 wins: 51.00\tmodel_2 wins: 50.00                                                                                                                     \n",
      "Episode 200\tmodel_1 wins: 111.00\tmodel_2 wins: 90.00                                                                                                                                 \n",
      "Episode 300\tmodel_1 wins: 172.00\tmodel_2 wins: 129.00                                                                                                                                           \n",
      "Episode 400\tmodel_1 wins: 215.00\tmodel_2 wins: 186.00                                                                                                                                                                                    \n",
      "Episode 500\tmodel_1 wins: 248.00\tmodel_2 wins: 253.00                                                                                                                        \n",
      "Episode 600\tmodel_1 wins: 284.00\tmodel_2 wins: 317.00                                                                                                                                                                                              \n",
      "Episode 700\tmodel_1 wins: 301.00\tmodel_2 wins: 400.00                                                                                                                                  \n",
      "Episode 800\tmodel_1 wins: 316.00\tmodel_2 wins: 485.00                                                                                                                        \n",
      "Episode 900\tmodel_1 wins: 328.00\tmodel_2 wins: 573.00                                                                                                                                            \n",
      "Episode 1000\tmodel_1 wins: 346.00\tmodel_2 wins: 655.00                                                                                                                                           \n",
      "Episode 1100\tmodel_1 wins: 375.00\tmodel_2 wins: 725.00                                                                                                    \n",
      "Episode 1200\tmodel_1 wins: 415.00\tmodel_2 wins: 785.00                                                                                                                                            \n",
      "Episode 1300\tmodel_1 wins: 448.00\tmodel_2 wins: 852.00                                                                                                                        \n",
      "Episode 1400\tmodel_1 wins: 500.00\tmodel_2 wins: 900.00                                                                                                              \n",
      "Episode 1500\tmodel_1 wins: 561.00\tmodel_2 wins: 938.00                                                                                \n",
      "Episode 1600\tmodel_1 wins: 604.00\tmodel_2 wins: 995.00                                                                                                    \n",
      "Episode 1700\tmodel_1 wins: 640.00\tmodel_2 wins: 1059.00                                                                                                              \n",
      "Episode 1800\tmodel_1 wins: 688.00\tmodel_2 wins: 1111.00                                                                                                                        \n",
      "Episode 1900\tmodel_1 wins: 731.00\tmodel_2 wins: 1168.00                                                                      \n",
      "Episode 2000\tmodel_1 wins: 785.00\tmodel_2 wins: 1214.00                                                            \n",
      "Episode 2100\tmodel_1 wins: 827.00\tmodel_2 wins: 1271.00                                                                                                    \n",
      "Episode 2200\tmodel_1 wins: 876.00\tmodel_2 wins: 1322.00                                                                                \n",
      "Episode 2300\tmodel_1 wins: 922.00\tmodel_2 wins: 1374.00                                                                                                                                                                                    \n",
      "Episode 2400\tmodel_1 wins: 966.00\tmodel_2 wins: 1430.00                                                                                                                                  \n",
      "Episode 2500\tmodel_1 wins: 1013.00\tmodel_2 wins: 1482.00                                                                                         \n",
      "Episode 2600\tmodel_1 wins: 1055.00\tmodel_2 wins: 1540.00                                                                                \n",
      "Episode 2700\tmodel_1 wins: 1094.00\tmodel_2 wins: 1601.00          \n",
      "Episode 2800\tmodel_1 wins: 1123.00\tmodel_2 wins: 1672.00                                                  \n",
      "Episode 2900\tmodel_1 wins: 1167.00\tmodel_2 wins: 1727.00                                                            \n",
      "Episode 3000\tmodel_1 wins: 1218.00\tmodel_2 wins: 1776.00                                                  \n",
      "Episode 3100\tmodel_1 wins: 1262.00\tmodel_2 wins: 1832.00                                                                                                                        \n",
      "Episode 3200\tmodel_1 wins: 1308.00\tmodel_2 wins: 1886.00                                                  \n",
      "Episode 3300\tmodel_1 wins: 1360.00\tmodel_2 wins: 1934.00                                                  \n",
      "Episode 3400\tmodel_1 wins: 1408.00\tmodel_2 wins: 1986.00                                                            \n",
      "Episode 3500\tmodel_1 wins: 1433.00\tmodel_2 wins: 2061.00                                                                                                    \n",
      "Episode 3600\tmodel_1 wins: 1468.00\tmodel_2 wins: 2126.00                                                                                                                        \n",
      "Episode 3700\tmodel_1 wins: 1518.00\tmodel_2 wins: 2176.00                                                                                                                                  \n",
      "Episode 3800\tmodel_1 wins: 1579.00\tmodel_2 wins: 2215.00                                                                                                                                                      \n",
      "Episode 3900\tmodel_1 wins: 1638.00\tmodel_2 wins: 2256.00                                                                                                                                                      \n",
      "Episode 4000\tmodel_1 wins: 1708.00\tmodel_2 wins: 2286.00                                                                                                                                  \n",
      "Episode 4100\tmodel_1 wins: 1756.00\tmodel_2 wins: 2338.00                                                                                                                                  \n",
      "Episode 4200\tmodel_1 wins: 1802.00\tmodel_2 wins: 2392.00                                                                                                                        \n",
      "Episode 4300\tmodel_1 wins: 1855.00\tmodel_2 wins: 2439.00                                                                                                                                                                                    \n",
      "Episode 4400\tmodel_1 wins: 1906.00\tmodel_2 wins: 2487.00                                                                                                                                                      \n",
      "Episode 4500\tmodel_1 wins: 1957.00\tmodel_2 wins: 2536.00                                                                                                              \n",
      "Episode 4600\tmodel_1 wins: 2002.00\tmodel_2 wins: 2591.00                                                                                                                                  \n",
      "Episode 4700\tmodel_1 wins: 2045.00\tmodel_2 wins: 2648.00                                                                                                                                  \n",
      "Episode 4800\tmodel_1 wins: 2074.00\tmodel_2 wins: 2719.00                                                                                                                                            \n",
      "Episode 4900\tmodel_1 wins: 2077.00\tmodel_2 wins: 2816.00                                                                                          \n",
      "Episode 5000\tmodel_1 wins: 2082.00\tmodel_2 wins: 2911.00                                                                                \n",
      "Episode 5100\tmodel_1 wins: 2082.00\tmodel_2 wins: 3011.00                                                                                                    \n",
      "Episode 5200\tmodel_1 wins: 2082.00\tmodel_2 wins: 3111.00                                                                                                              \n",
      "Episode 5300\tmodel_1 wins: 2084.00\tmodel_2 wins: 3209.00                                                                                                    \n",
      "Episode 5400\tmodel_1 wins: 2085.00\tmodel_2 wins: 3308.00                                                                                          \n",
      "Episode 5500\tmodel_1 wins: 2085.00\tmodel_2 wins: 3408.00                                                                                \n",
      "Episode 5600\tmodel_1 wins: 2088.00\tmodel_2 wins: 3505.00                                                                                \n",
      "Episode 5700\tmodel_1 wins: 2136.00\tmodel_2 wins: 3557.00                                                                                          \n",
      "Episode 5800\tmodel_1 wins: 2216.00\tmodel_2 wins: 3577.00                                                                                                              \n",
      "Episode 5900\tmodel_1 wins: 2278.00\tmodel_2 wins: 3615.00                                                                                                                                                      \n",
      "Episode 6000\tmodel_1 wins: 2323.00\tmodel_2 wins: 3670.00                                                                                          \n",
      "Episode 6100\tmodel_1 wins: 2382.00\tmodel_2 wins: 3711.00                                                                                                                                  \n",
      "Episode 6200\tmodel_1 wins: 2436.00\tmodel_2 wins: 3757.00                                                                                          \n",
      "Episode 6300\tmodel_1 wins: 2462.00\tmodel_2 wins: 3831.00                                                                                                                                                      \n",
      "Episode 6400\tmodel_1 wins: 2501.00\tmodel_2 wins: 3892.00                                                                                                                                            \n",
      "Episode 6500\tmodel_1 wins: 2554.00\tmodel_2 wins: 3939.00                                                                                                                                            \n",
      "Episode 6600\tmodel_1 wins: 2613.00\tmodel_2 wins: 3980.00                                                  \n",
      "Episode 6700\tmodel_1 wins: 2679.00\tmodel_2 wins: 4014.00                                                                                          \n",
      "Episode 6800\tmodel_1 wins: 2735.00\tmodel_2 wins: 4058.00                                                                                                                        \n",
      "Episode 6900\tmodel_1 wins: 2801.00\tmodel_2 wins: 4092.00                                                                      \n",
      "Episode 7000\tmodel_1 wins: 2857.00\tmodel_2 wins: 4136.00                                                                                                                                  \n",
      "Episode 7100\tmodel_1 wins: 2925.00\tmodel_2 wins: 4168.00                                                                                          \n",
      "Episode 7200\tmodel_1 wins: 2993.00\tmodel_2 wins: 4200.00                                                                                                                                                      \n",
      "Episode 7300\tmodel_1 wins: 3060.00\tmodel_2 wins: 4233.00                                                                                                              \n",
      "Episode 7400\tmodel_1 wins: 3122.00\tmodel_2 wins: 4271.00                                                                                                              \n",
      "Episode 7500\tmodel_1 wins: 3184.00\tmodel_2 wins: 4309.00                                                                                          \n",
      "Episode 7600\tmodel_1 wins: 3249.00\tmodel_2 wins: 4344.00                                                                                                                                                      \n",
      "Episode 7700\tmodel_1 wins: 3312.00\tmodel_2 wins: 4381.00                                                                                                                                                      \n",
      "Episode 7800\tmodel_1 wins: 3382.00\tmodel_2 wins: 4411.00                                                                                                                        \n",
      "Episode 7900\tmodel_1 wins: 3438.00\tmodel_2 wins: 4455.00                                                                                                                                                                \n",
      "Episode 8000\tmodel_1 wins: 3464.00\tmodel_2 wins: 4529.00                                                                                \n",
      "Episode 8100\tmodel_1 wins: 3527.00\tmodel_2 wins: 4566.00                                                                                                                                                                \n",
      "Episode 8200\tmodel_1 wins: 3575.00\tmodel_2 wins: 4618.00                                                                                                                        \n",
      "Episode 8300\tmodel_1 wins: 3627.00\tmodel_2 wins: 4666.00                                                                                                    \n",
      "Episode 8400\tmodel_1 wins: 3676.00\tmodel_2 wins: 4717.00                                                            \n",
      "Episode 8500\tmodel_1 wins: 3714.00\tmodel_2 wins: 4779.00                                                                      \n",
      "Episode 8600\tmodel_1 wins: 3760.00\tmodel_2 wins: 4833.00                                                                                                                                                                \n",
      "Episode 8700\tmodel_1 wins: 3787.00\tmodel_2 wins: 4906.00                                                                                                              \n",
      "Episode 8800\tmodel_1 wins: 3819.00\tmodel_2 wins: 4974.00                                                  \n",
      "Episode 8900\tmodel_1 wins: 3867.00\tmodel_2 wins: 5026.00                                                                                                    \n",
      "Episode 9000\tmodel_1 wins: 3909.00\tmodel_2 wins: 5084.00                                                            \n",
      "Episode 9100\tmodel_1 wins: 3939.00\tmodel_2 wins: 5154.00                                                                                                                                            \n",
      "Episode 9200\tmodel_1 wins: 3971.00\tmodel_2 wins: 5222.00                                                                                                                                            \n",
      "Episode 9300\tmodel_1 wins: 4008.00\tmodel_2 wins: 5285.00                                                                                                                        \n",
      "Episode 9400\tmodel_1 wins: 4040.00\tmodel_2 wins: 5353.00                                                                                                                        \n",
      "Episode 9500\tmodel_1 wins: 4084.00\tmodel_2 wins: 5409.00                                                                                                                        \n",
      "Episode 9600\tmodel_1 wins: 4136.00\tmodel_2 wins: 5457.00                                                                                                                        \n",
      "Episode 9700\tmodel_1 wins: 4176.00\tmodel_2 wins: 5517.00                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9800\tmodel_1 wins: 4218.00\tmodel_2 wins: 5575.00                                                                                                                        \n",
      "Episode 9900\tmodel_1 wins: 4265.00\tmodel_2 wins: 5628.00                                                                                                                                  \n",
      "Episode 9999\tmodel_1 wins: 4299.00\tmodel_2 wins: 5693.00                                                                                                                                            "
     ]
    }
   ],
   "source": [
    "t_max = 500\n",
    "episodes_max = 10000\n",
    "train(episodes_max, t_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  0.,  1.]), array([5731,   32, 4211], dtype=int64))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(model_1.history), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1.wins = 4211; games_played = 9974\n",
      "model_2.wins = 5731; games_played = 9974\n"
     ]
    }
   ],
   "source": [
    "print(f'{model_1.wins = }; games_played = {model_1.games_played}')\n",
    "print(f'{model_2.wins = }; games_played = {model_1.games_played}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There. we finished\n",
    "### Lets see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['action_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['observation'].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl1_wins = 1\tpl2_wins = 4\n"
     ]
    }
   ],
   "source": [
    "pl1_wins = 0\n",
    "pl2_wins = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        env.reset()\n",
    "        reward = [0,0]\n",
    "        done = [0,0]\n",
    "        for t in range(100):\n",
    "            state, reward[t%2], done[t%2], _ = env.last()\n",
    "            env.render()\n",
    "            if done[t%2]:\n",
    "                if all(done):\n",
    "                    locals()[f'pl{t%2+1}_wins'] += 1 if reward[t%2] > 0 else 0\n",
    "                    locals()[f'pl{(t+1)%2+1}_wins'] += 1 if reward[(t+1)%2] > 0 else 0\n",
    "                    break\n",
    "                env.step(None)\n",
    "                continue\n",
    "            action = players[t%2].select_action(state['observation'], state['action_mask'])\n",
    "            env.step(action)\n",
    "            sleep(0.1)\n",
    "    env.close()\n",
    "print(f'{pl1_wins = }\\t{pl2_wins = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players[0].select_action(state['observation'].flatten(), state['action_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[1 0]\n",
      "  [0 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]]\n",
      "(6, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, _ = env.last()\n",
    "print(state['observation'])\n",
    "print(state['observation'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
